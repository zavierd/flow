"""
å±æ€§åˆ†æå™¨
è´Ÿè´£è¯†åˆ«å’Œåˆ†ææœªå®šä¹‰çš„äº§å“å±æ€§
"""

import json
import logging
from typing import Dict, Any, List, Optional
from .deepseek_service import DeepSeekService

logger = logging.getLogger(__name__)


class AttributeAnalyzer:
    """å±æ€§åˆ†æå™¨ - æ™ºèƒ½è¯†åˆ«å’Œå¤„ç†æœªå®šä¹‰å±æ€§"""
    
    def __init__(self):
        self.ai_service = DeepSeekService()

        # ä»é…ç½®è·å–å‚æ•°
        from products.config.smart_attribute_config import SMART_ATTRIBUTE_CONFIG
        self.confidence_threshold = SMART_ATTRIBUTE_CONFIG.get('confidence_threshold', 0.6)
        self.use_real_ai = SMART_ATTRIBUTE_CONFIG.get('use_real_ai', True)
        
    def identify_unknown_attributes(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è¯†åˆ«æœªå®šä¹‰çš„å±æ€§"""
        from products.config.ai_data_mapping import PRODUCT_STRUCTURE_MAPPING
        
        # è·å–å·²çŸ¥å­—æ®µ
        known_fields = set(PRODUCT_STRUCTURE_MAPPING['attribute_fields'])
        
        # æ·»åŠ åŸºç¡€å­—æ®µï¼ˆä¸åº”ä½œä¸ºå±æ€§å¤„ç†ï¼‰
        base_fields = {
            'äº§å“æè¿°', 'äº§å“ç¼–ç ', 'å¤‡æ³¨',
            'ä»·æ ¼ç­‰çº§I', 'ä»·æ ¼ç­‰çº§II', 'ä»·æ ¼ç­‰çº§III', 'ä»·æ ¼ç­‰çº§IV', 'ä»·æ ¼ç­‰çº§V'
        }
        
        excluded_fields = known_fields.union(base_fields)
        
        # è¯†åˆ«æœªçŸ¥å±æ€§
        unknown_attributes = {}
        for key, value in data.items():
            if (key not in excluded_fields and 
                value and 
                str(value).strip() and
                str(value).strip() not in ['', '-', 'N/A', '0', '0.0']):
                unknown_attributes[key] = value
        
        logger.info(f"ğŸ” è¯†åˆ«åˆ° {len(unknown_attributes)} ä¸ªæœªå®šä¹‰å±æ€§: {list(unknown_attributes.keys())}")
        return unknown_attributes
    
    def analyze_attributes_batch(self, unknown_attributes: Dict[str, Any], context_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†ææœªçŸ¥å±æ€§"""
        analyzed_attributes = []
        
        for attr_name, attr_value in unknown_attributes.items():
            try:
                analysis_result = self.analyze_single_attribute(attr_name, attr_value, context_data)
                if analysis_result:
                    analyzed_attributes.append(analysis_result)
                    logger.info(f"ğŸ¤– AIåˆ†æå±æ€§: {attr_name} â†’ {analysis_result['display_name']}")
                
            except Exception as e:
                logger.warning(f"åˆ†æå±æ€§å¤±è´¥ {attr_name}: {str(e)}")
                # ä½¿ç”¨é»˜è®¤åˆ†æ
                analyzed_attributes.append(self._create_default_analysis(attr_name, attr_value))
        
        return analyzed_attributes
    
    def analyze_single_attribute(self, attr_name: str, attr_value: str, context_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """åˆ†æå•ä¸ªå±æ€§"""
        if not self.ai_service.is_available():
            logger.info(f"AIæœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤åˆ†æå¤„ç†å±æ€§: {attr_name}")
            return self._create_default_analysis(attr_name, attr_value)
        
        # æ„å»ºAIåˆ†ææç¤ºè¯
        prompt = self._build_analysis_prompt(attr_name, attr_value, context_data)
        
        # è°ƒç”¨AIæœåŠ¡
        ai_response = self.ai_service.generate_response(prompt)
        
        # è§£æAIå“åº”
        analysis_result = self._parse_ai_response(ai_response, attr_name, attr_value)
        
        # éªŒè¯åˆ†æç»“æœ
        if self._validate_analysis_result(analysis_result):
            return analysis_result
        else:
            logger.warning(f"AIåˆ†æç»“æœéªŒè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ: {attr_name}")
            return self._create_default_analysis(attr_name, attr_value)
    
    def _build_analysis_prompt(self, attr_name: str, attr_value: str, context_data: Dict[str, Any]) -> str:
        """æ„å»ºAIåˆ†ææç¤ºè¯"""
        product_desc = context_data.get('äº§å“æè¿°', '')
        series = context_data.get('ç³»åˆ—', '')
        type_code = context_data.get('ç±»å‹ä»£ç ', '')

        prompt = f"""è¯·åˆ†æå®¶å…·äº§å“å±æ€§å¹¶è¿”å›JSONæ ¼å¼ç»“æœã€‚

å±æ€§å: {attr_name}
å±æ€§å€¼: {attr_value}
äº§å“: {product_desc}

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- display_name: æ ‡å‡†åŒ–å±æ€§åç§°
- display_value: æ ‡å‡†åŒ–å±æ€§å€¼
- attribute_type: æ•°æ®ç±»å‹(text/number/select/boolean/color)
- filterable: æ˜¯å¦å¯ç­›é€‰(true/false)
- importance: é‡è¦ç¨‹åº¦(1-5)
- confidence: ç½®ä¿¡åº¦(0.0-1.0)

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š"""
        return prompt
    
    def _parse_ai_response(self, ai_response: str, attr_name: str, attr_value: str) -> Dict[str, Any]:
        """è§£æAIå“åº”"""
        try:
            # è®°å½•åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            logger.debug(f"AIåŸå§‹å“åº”: {repr(ai_response)}")

            # æ¸…ç†å“åº”å†…å®¹
            cleaned_response = ai_response.strip()

            # å°è¯•æå–JSONéƒ¨åˆ†ï¼ˆå¦‚æœå“åº”åŒ…å«å…¶ä»–æ–‡æœ¬ï¼‰
            import re
            json_match = re.search(r'\{.*\}', cleaned_response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                logger.debug(f"æå–çš„JSON: {json_str}")
            else:
                json_str = cleaned_response

            # å°è¯•è§£æJSON
            result = json.loads(json_str)

            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            required_fields = ['display_name', 'display_value', 'attribute_type', 'filterable', 'importance']
            for field in required_fields:
                if field not in result:
                    raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")

            # æ·»åŠ åŸå§‹ä¿¡æ¯
            result['original_name'] = attr_name
            result['original_value'] = attr_value

            logger.info(f"âœ… AIå“åº”è§£ææˆåŠŸ: {attr_name} -> {result['display_name']}")
            return result

        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"è§£æAIå“åº”å¤±è´¥: {str(e)}")
            logger.warning(f"åŸå§‹å“åº”å†…å®¹: {repr(ai_response)}")
            return self._create_default_analysis(attr_name, attr_value)
    
    def _validate_analysis_result(self, result: Dict[str, Any]) -> bool:
        """éªŒè¯åˆ†æç»“æœçš„æœ‰æ•ˆæ€§"""
        try:
            # æ£€æŸ¥ç½®ä¿¡åº¦
            confidence = result.get('confidence', 0)
            if confidence < self.confidence_threshold:
                logger.info(f"AIåˆ†æç½®ä¿¡åº¦è¿‡ä½: {confidence} < {self.confidence_threshold}")
                return False
            
            # æ£€æŸ¥å±æ€§ç±»å‹
            valid_types = ['text', 'number', 'select', 'boolean', 'color']
            if result.get('attribute_type') not in valid_types:
                return False
            
            # æ£€æŸ¥é‡è¦ç¨‹åº¦
            importance = result.get('importance', 0)
            if not isinstance(importance, int) or importance < 1 or importance > 5:
                return False
            
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_fields = ['display_name', 'display_value']
            for field in required_fields:
                if not result.get(field) or not str(result[field]).strip():
                    return False
            
            return True
            
        except Exception as e:
            logger.warning(f"éªŒè¯åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
            return False
    
    def _create_default_analysis(self, attr_name: str, attr_value: str) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤åˆ†æç»“æœ"""
        return {
            'original_name': attr_name,
            'original_value': attr_value,
            'display_name': attr_name,
            'display_value': str(attr_value),
            'attribute_type': 'text',
            'filterable': False,
            'importance': 3,
            'confidence': 0.5,
            'source': 'default'  # æ ‡è®°ä¸ºé»˜è®¤å¤„ç†
        }
    
    def get_analysis_summary(self, analyzed_attributes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è·å–åˆ†ææ‘˜è¦"""
        if not analyzed_attributes:
            return {'total': 0, 'ai_processed': 0, 'default_processed': 0}
        
        total = len(analyzed_attributes)
        ai_processed = sum(1 for attr in analyzed_attributes if attr.get('source') != 'default')
        default_processed = total - ai_processed
        
        avg_confidence = sum(attr.get('confidence', 0) for attr in analyzed_attributes) / total
        high_importance = sum(1 for attr in analyzed_attributes if attr.get('importance', 0) >= 4)
        filterable_count = sum(1 for attr in analyzed_attributes if attr.get('filterable', False))
        
        return {
            'total': total,
            'ai_processed': ai_processed,
            'default_processed': default_processed,
            'average_confidence': round(avg_confidence, 2),
            'high_importance_count': high_importance,
            'filterable_count': filterable_count,
            'attribute_types': self._count_attribute_types(analyzed_attributes)
        }
    
    def _count_attribute_types(self, analyzed_attributes: List[Dict[str, Any]]) -> Dict[str, int]:
        """ç»Ÿè®¡å±æ€§ç±»å‹åˆ†å¸ƒ"""
        type_counts = {}
        for attr in analyzed_attributes:
            attr_type = attr.get('attribute_type', 'unknown')
            type_counts[attr_type] = type_counts.get(attr_type, 0) + 1
        return type_counts
